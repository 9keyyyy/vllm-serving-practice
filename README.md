# vllm-serving-practice

## í˜„ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
ë¡œì»¬ í™˜ê²½(Macbook Pro M4)ì—ì„œì˜ í…ŒìŠ¤íŠ¸ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì•„ë˜ì²˜ëŸ¼ êµ¬ì„± -> ì¶”í›„ ê³ ë„í™” ì˜ˆì •
```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚           Internet            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚
        â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Colab     â”‚    â”‚   Kubernetes Cluster           â”‚
â”‚              â”‚    â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  vLLM   â”‚ â”‚    â”‚  â”‚ InferenceService         â”‚  â”‚
â”‚  â”‚  Server â”‚ â”‚    â”‚  â”‚   (llm-api)              â”‚  â”‚
â”‚  â”‚         â”‚ â”‚    â”‚  â”‚                          â”‚  â”‚
â”‚  â”‚ Phi-3   â”‚ â”‚â—„â”€â”€â”€â”¼â”€â”€â”‚  FastAPI                 â”‚  â”‚
â”‚  â”‚ (GPU)   â”‚ â”‚    â”‚  â”‚  - Agent                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â”‚  - BatchHandler          â”‚  â”‚
â”‚      â”‚       â”‚    â”‚  â”‚  - Monitoring            â”‚  â”‚
â”‚      â”‚       â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”‚    â”‚              â–²                 â”‚
â”‚  â”‚ ngrok  â”‚  â”‚    â”‚              â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚        Istio Gateway           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                            User Requests
```


## í´ë” êµ¬ì¡°
```
llm-serving-practice/
â”œâ”€â”€ .python-version          # Python 3.13
â”œâ”€â”€ pyproject.toml          # uv ì˜ì¡´ì„± ê´€ë¦¬
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.api      # FastAPI ì„œë²„ìš©
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ llm-api-inference.yaml
â”‚   â””â”€â”€ monitoring/  # TODO
â”‚       â”œâ”€â”€ prometheus.yaml
â”‚       â””â”€â”€ grafana-dashboard.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py      # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vllm_client.py  # vLLM API í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ batch_handler.py # ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§
â”‚   â”‚   â””â”€â”€ agent.py         # TODO
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py         # FastAPI ì•±
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ batch.py
â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â””â”€â”€ middleware/
â”‚   â”‚       â””â”€â”€ metrics.py  # TODO: Prometheus metrics
â”œâ”€â”€ tests/ 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_vllm_client.py
â”‚   â”œâ”€â”€ test_batch.py
â”‚   â””â”€â”€ benchmarks/
â”‚       â””â”€â”€ batch_performance.py
â””â”€â”€ notebooks/
    â””â”€â”€ 01_vllm_setup.ipynb      # vllm server (Colab)
```

## vLLM ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì„¤ê³„ ë° ì„±ëŠ¥ ê²€ì¦

> vLLM Continuous Batchingì„ ê²€ì¦í•˜ê³ , ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì„ ìœ„í•œ ìë™ ë°°ì¹˜ ì‹œìŠ¤í…œ ì„¤ê³„

### ì²˜ë¦¬ í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Client Requests                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ add_request()
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BatchHandler                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Request Queue (deque)                             â”‚     â”‚
â”‚  â”‚  [(requestâ‚, futureâ‚), (requestâ‚‚, futureâ‚‚), ...]   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                        â”‚                                    â”‚
â”‚                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Batch Processor                                   â”‚     â”‚
â”‚  â”‚  â€¢ timeout_ms: 100ms ëŒ€ê¸°                           â”‚     â”‚
â”‚  â”‚  â€¢ max_batch_size: 32ê°œ ì œí•œ                         â”‚     â”‚
â”‚  â”‚  â€¢ ì¡°ê±´ ì¶©ì¡± ì‹œ ë°°ì¹˜ í˜•ì„±                               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ batch_chat_completion()
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    vLLM Engine                              â”‚
â”‚                 (Continuous Batching)                       â”‚
â”‚                                                             â”‚
â”‚  â€¢ GPU ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë‹¤ì¤‘ ìš”ì²­ ë™ì‹œ ì‹¤í–‰                             â”‚
â”‚  â€¢ í† í° ê¸¸ì´ ë¬´ê´€í•˜ê²Œ íš¨ìœ¨ì  ì²˜ë¦¬                                  â”‚
â”‚  â€¢ ì™„ë£Œëœ ìš”ì²­ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ë°˜í™˜                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


### í…ŒìŠ¤íŠ¸ í™˜ê²½
- **ìš”ì²­ ìˆ˜**: 50ê°œ
- **í† í° ë²”ìœ„**: 10~150 tokens 
- **í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**: ë™ì‹œ ìš”ì²­ í™˜ê²½ (ì„±ëŠ¥ ê²€ì¦ìš©)

### ì²˜ë¦¬ ë°©ì‹ë³„ ì„±ëŠ¥ ë¹„êµ

| ì²˜ë¦¬ ë°©ì‹ | ì†Œìš” ì‹œê°„ | Throughput  | ì„±ëŠ¥ ê°œì„  |
|---------|---------|------------|---------|
| ìˆœì°¨ ì²˜ë¦¬ (Baseline) | 129.05s | 0.39 req/s  | 1.0x |
| **ë™ì‹œ ìš”ì²­** | 11.41s | 4.38 req/s  | **11.3x** â¬†ï¸ |
| ëª…ì‹œì  ë°°ì¹˜ | 11.45s | 4.37 req/s  | 11.3x |
| **BatchHandler** | 17.89s | 2.80 req/s  | **7.2x** â¬†ï¸ |

<details>
<summary><b>ìƒì„¸ ê²°ê³¼</b></summary>

```
âœ… vLLM server connected

============================================================

ğŸ“Š Generated 50 varied requests
   Token range: 10-150 tokens

=== Single Requests Benchmark (n=50) ===
Request 10/50 completed
Request 20/50 completed
Request 30/50 completed
Request 40/50 completed
Request 50/50 completed

Results:
Total time: 129.05s
Throughput: 0.39 req/s
Avg latency: 2.58s
P50 latency: 1.37s
P95 latency: 5.45s

=== Concurrent Requests Benchmark (n=50) ===

Results:
Total time: 11.41s
Throughput: 4.38 req/s
Avg latency: 5.43s
P50 latency: 4.85s
P95 latency: 10.74s

=== Explicit Batch Benchmark (n=50) ===

Results:
Total time: 11.45s
Throughput: 4.37 req/s
Avg latency: 5.15s

=== Batch Handler Benchmark (n=50) ===

Results:
Total time: 17.89s
Throughput: 2.80 req/s

Batch Stats:
Total requests: 50
Total batches: 2
Avg batch size: 25.00

============================================================
SUMMARY
============================================================
Single requests (est): 129.05s (baseline)
Concurrent requests:   11.41s (11.31x faster)
Explicit batch:        11.45s (11.27x faster)
Batch handler:         17.89s (7.21x faster)
```

</details>

### ê²€ì¦ ê²°ê³¼ 
- vLLM Continuous Batching: 11.3ë°°ì˜ ì²˜ë¦¬ëŸ‰ í–¥ìƒì„ ì œê³µ
- BatchHandler: 7.2ë°°ì˜ ì²˜ë¦¬ëŸ‰ í–¥ìƒì„ ì œê³µ ë° 50ê°œ ìš”ì²­ì„ 2íšŒ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì™„ë£Œ

### í•µì‹¬ êµ¬í˜„: BatchHandler
ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ìš”ì²­ì´ ìˆœì°¨ì ìœ¼ë¡œ ë„ì°©í•˜ê¸° ë•Œë¬¸ì—, BatchHandlerëŠ” ì´ëŸ° í™˜ê²½ì—ì„œë„ ìë™ìœ¼ë¡œ ìš”ì²­ì„ ìˆ˜ì§‘í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë„ë¡ ì„¤ê³„
1. **`collections.deque`**: íš¨ìœ¨ì ì¸ FIFO í êµ¬í˜„
2. **ì‹œê°„ ê¸°ë°˜ ë°°ì¹˜ í˜•ì„±**: `timeout_ms` ëŒ€ê¸°ë¡œ ì§€ì—° ìµœì†Œí™”
3. **í¬ê¸° ì œí•œ**: `max_batch_size`ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë³´ì¥
4. **Future íŒ¨í„´**: ë¹„ë™ê¸° ì‘ë‹µ ì²˜ë¦¬ ë° ì—ëŸ¬ í•¸ë“¤ë§

**ë¹„ë™ê¸° í ê¸°ë°˜ ìš”ì²­ ìˆ˜ì§‘**
```python
async def add_request(self, request: ChatRequest) -> ChatResponse:
    """ìš”ì²­ì„ íì— ì¶”ê°€í•˜ê³  ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ëŒ€ê¸°"""
    future = asyncio.Future()
    self.queue.append((request, future))
    
    # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì‹œì‘
    if not self._processing:
        asyncio.create_task(self._process_batch())
    
    return await future
```

**ì‹œê°„/í¬ê¸° ê¸°ë°˜ ë°°ì¹˜ í˜•ì„±**
```python
async def _process_batch(self):
    """ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§"""
    # timeout_ms ëŒ€ê¸° í›„ íì—ì„œ ë°°ì¹˜ ì¶”ì¶œ
    await asyncio.sleep(self.timeout_ms / 1000)
    
    batch = []
    futures = []
    
    # max_batch_sizeê¹Œì§€ ìš”ì²­ ìˆ˜ì§‘
    while self.queue and len(batch) < self.max_batch_size:
        request, future = self.queue.popleft()
        batch.append(request)
        futures.append(future)
    
    # vLLMìœ¼ë¡œ ë°°ì¹˜ ì „ì†¡
    responses = await self.client.batch_chat_completion(batch)
    
    # Futureë¥¼ í†µí•´ ê° ìš”ì²­ìì—ê²Œ ì‘ë‹µ ë°˜í™˜
    for future, response in zip(futures, responses):
        future.set_result(response)
```




### Application Layer (BatchHandler)
> ì—¬ëŸ¬ ê°œë³„ ìš”ì²­ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ í†µí•©

- API í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œ (50íšŒ â†’ 2íšŒ)
- ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- ê°œë°œìëŠ” ë‹¨ì¼ ìš”ì²­ API ì‚¬ìš© (ì¶”ìƒí™”)

### Engine Layer (vLLM Continuous Batching)
> ë°°ì¹˜ë¡œ ì „ë‹¬ëœ ìš”ì²­ë“¤ì„ GPUì—ì„œ ë³‘ë ¬ ì²˜ë¦¬

- ë‹¤ì–‘í•œ í† í° ê¸¸ì´(10~150) ë™ì‹œ ì²˜ë¦¬
- ì§§ì€ ìš”ì²­ ì™„ë£Œ í›„ì—ë„ ê¸´ ìš”ì²­ ê³„ì† ì‹¤í–‰
- GPU ìœ íœ´ ì‹œê°„ ìµœì†Œí™”ë¡œ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”

### Application Layer + Engine Layer
```
BatchHandler
  â””â”€â†’ ìš”ì²­ ìë™ ìˆ˜ì§‘ ë° ë°°ì¹˜ í˜•ì„±
        â””â”€â†’ vLLM Engine
              â””â”€â†’ GPU ë³‘ë ¬ ì²˜ë¦¬ (Continuous Batching)
                    â””â”€â†’ 11.3ë°° ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±
```




## TODO
- [x] vLLM ì„œë²„ ì„¸íŒ…
- [x] FastAPI ì„œë²„ ì„¸íŒ… ë° ëª¨ë¸ ì—°ë™ í™•ì¸
- [x] KServe ì„œë¹™
- [x] continuous batching/batch handler ì„±ëŠ¥ í‰ê°€
- [ ] Prometheus + Grafana ëª¨ë‹ˆí„°ë§
- [ ] LangChain Agent êµ¬í˜„
- [ ] í´ë¼ìš°ë“œ í™˜ê²½ GPU í™œìš© vLLM ì„œë²„ ì„¸íŒ…