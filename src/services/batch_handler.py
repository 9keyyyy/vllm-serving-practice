import asyncio
import logging
import time
from collections import deque
from typing import Dict

from src.api.middleware.metrics import record_batch_metrics
from src.config import settings
from src.models.schemas import ChatRequest, ChatResponse
from src.services.vllm_client import VLLMClient

logger = logging.getLogger(__name__)


class BatchHandler:
    """
    ìš”ì²­ì„ ëª¨ì•„ì„œ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” í•¸ë“¤ëŸ¬

    vLLMì˜ continuous batchingì„ ìµœëŒ€í•œ í™œìš©í•˜ê¸° ìœ„í•´,
    vLLM Engineì˜ max-num-seqs ê°’ê³¼ batch_max_size ê°’ì„ ë™ì¼í•˜ê²Œ ì„¤ì • í•„ìš”
    """

    def __init__(self, vllm_client: VLLMClient):
        self.client = vllm_client
        self.queue: deque = deque()
        self.max_batch_size = settings.batch_max_size
        self.timeout_ms = settings.batch_timeout_ms
        self._processing = False
        self._stats = {
            "total_requests": 0,
            "total_batches": 0,
            "avg_batch_size": 0.0,
        }

    async def add_request(self, request: ChatRequest) -> ChatResponse:
        """
        ìš”ì²­ì„ íì— ì¶”ê°€í•˜ê³  ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ëŒ€ê¸°
        """
        future = asyncio.Future()
        self.queue.append((request, future))
        self._stats["total_requests"] += 1

        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ì—†ìœ¼ë©´)
        if not self._processing:
            asyncio.create_task(self._process_batch())

        # ê²°ê³¼ ëŒ€ê¸°
        return await future

    async def _process_batch(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§"""
        if self._processing:
            return

        self._processing = True

        try:
            # íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ìµœëŒ€ ë°°ì¹˜ í¬ê¸°ê¹Œì§€ ëŒ€ê¸°
            await asyncio.sleep(self.timeout_ms / 1000)

            if not self.queue:
                return

            # íì—ì„œ ë°°ì¹˜ ì¶”ì¶œ
            batch = []
            futures = []

            while self.queue and len(batch) < self.max_batch_size:
                request, future = self.queue.popleft()
                batch.append(request)
                futures.append(future)

            if not batch:
                return

            batch_size = len(batch)
            logger.info(f"Processing batch of {batch_size} requests")

            # vLLMìœ¼ë¡œ ë°°ì¹˜ ì „ì†¡
            start_time = time.perf_counter()
            responses = await self.client.batch_chat_completion(batch)
            total_time = (time.perf_counter() - start_time) * 1000

            # ğŸ“Š ë©”íŠ¸ë¦­ ê¸°ë¡
            record_batch_metrics(
                batch_size=batch_size, duration_seconds=total_time / 1000
            )

            # í†µê³„ ì—…ë°ì´íŠ¸
            self._stats["total_batches"] += 1
            self._stats["avg_batch_size"] = (
                self._stats["avg_batch_size"] * (self._stats["total_batches"] - 1)
                + batch_size
            ) / self._stats["total_batches"]

            logger.info(
                f"Batch completed: {batch_size} requests in {total_time:.2f}ms "
                f"({batch_size / (total_time / 1000):.2f} req/s)"
            )

            # Futureì— ê²°ê³¼ ì „ë‹¬
            for future, response in zip(futures, responses):
                if not future.done():
                    future.set_result(response)

        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            # ì—ëŸ¬ ì‹œ ëª¨ë“  futureì— ì—ëŸ¬ ì „ë‹¬
            for _, future in list(self.queue):
                if not future.done():
                    future.set_exception(e)

        finally:
            self._processing = False

            # íì— ë‚¨ì€ ìš”ì²­ì´ ìˆìœ¼ë©´ ë‹¤ì‹œ ì²˜ë¦¬
            if self.queue:
                asyncio.create_task(self._process_batch())

    def get_stats(self) -> Dict:
        """ë°°ì¹˜ ì²˜ë¦¬ í†µê³„"""
        return self._stats.copy()
