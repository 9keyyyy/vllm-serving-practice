import asyncio
import logging
import time
from typing import List, Optional

from openai import AsyncOpenAI

from src.api.middleware.metrics import record_vllm_metrics
from src.config import settings
from src.models.schemas import ChatRequest, ChatResponse

logger = logging.getLogger(__name__)


class VLLMClient:
    """vLLM ì„œë²„ì™€ í†µì‹ í•˜ëŠ” ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.client = AsyncOpenAI(
            base_url=f"{settings.vllm_base_url}/v1",
            api_key=settings.vllm_api_key,
        )
        self.model = settings.vllm_model

    async def chat_completion(
        self, request: ChatRequest, request_id: Optional[str] = None
    ) -> ChatResponse:
        """ë‹¨ì¼ ì±„íŒ… ìš”ì²­ ì²˜ë¦¬"""
        start_time = time.perf_counter()

        try:
            completion = await self.client.chat.completions.create(
                model=self.model,
                messages=[msg.model_dump() for msg in request.messages],
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stream=False,
            )

            latency_ms = (time.perf_counter() - start_time) * 1000

            # ðŸ“Š ë©”íŠ¸ë¦­ ê¸°ë¡ - ì„±ê³µ
            record_vllm_metrics(success=True, latency_seconds=latency_ms / 1000)

            return ChatResponse(
                id=request_id or completion.id,
                response=completion.choices[0].message.content,
                model=completion.model,
                usage=completion.usage.model_dump(exclude_none=True),
                latency_ms=latency_ms,
            )

        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000

            # ðŸ“Š ë©”íŠ¸ë¦­ ê¸°ë¡ - ì‹¤íŒ¨
            record_vllm_metrics(success=False, latency_seconds=latency_ms / 1000)

            logger.error(f"vLLM request failed: {e}")
            raise

    async def batch_chat_completion(
        self, requests: List[ChatRequest]
    ) -> List[ChatResponse]:
        """
        ë°°ì¹˜ ìš”ì²­ ì²˜ë¦¬ - vLLMì˜ continuous batching í™œìš©
        ì—¬ëŸ¬ ìš”ì²­ì„ ë™ì‹œì— ë³´ë‚´ë©´ vLLMì´ ìžë™ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        """
        start_time = time.perf_counter()

        # ëª¨ë“  ìš”ì²­ì„ ë™ì‹œì— ì‹¤í–‰ (vLLMì´ ë‚´ë¶€ì ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬)
        tasks = [
            self.chat_completion(req, request_id=f"batch_{i}")
            for i, req in enumerate(requests)
        ]

        responses = await asyncio.gather(*tasks, return_exceptions=True)

        # ì—ëŸ¬ ì²˜ë¦¬
        valid_responses = []
        for i, resp in enumerate(responses):
            if isinstance(resp, Exception):
                logger.error(f"Batch request {i} failed: {resp}")
            else:
                valid_responses.append(resp)

        total_time = (time.perf_counter() - start_time) * 1000
        logger.info(
            f"Batch completed: {len(valid_responses)}/{len(requests)} requests "
            f"in {total_time:.2f}ms"
        )

        return valid_responses

    async def health_check(self) -> bool:
        """vLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = await self.client.models.list()
            return len(response.data) > 0
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False
